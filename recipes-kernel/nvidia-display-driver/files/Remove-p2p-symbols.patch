diff -urN NVIDIA-kernel-module-source-TempVersion.orig/kernel-open/nvidia/nv-p2p.c NVIDIA-kernel-module-source-TempVersion/kernel-open/nvidia/nv-p2p.c
--- NVIDIA-kernel-module-source-TempVersion.orig/kernel-open/nvidia/nv-p2p.c	2023-05-22 17:03:01.224734798 -0600
+++ NVIDIA-kernel-module-source-TempVersion/kernel-open/nvidia/nv-p2p.c	2023-05-22 17:17:35.926339707 -0600
@@ -327,448 +327,6 @@
     nv_p2p_free_platform_data(&mem_info->page_table);
 }
 
-int nvidia_p2p_get_pages(
-    uint64_t p2p_token,
-    uint32_t va_space,
-    uint64_t virtual_address,
-    uint64_t length,
-    struct nvidia_p2p_page_table **page_table,
-    void (*free_callback)(void * data),
-    void *data
-)
-{
-    NV_STATUS status;
-    nvidia_stack_t *sp = NULL;
-    struct nvidia_p2p_page *page;
-    struct nv_p2p_mem_info *mem_info = NULL;
-    NvU32 entries;
-    NvU32 *wreqmb_h = NULL;
-    NvU32 *rreqmb_h = NULL;
-    NvU64 *physical_addresses = NULL;
-    NvU32 page_count;
-    NvU32 i = 0;
-    NvBool bGetPages = NV_FALSE;
-    NvBool bGetUuid = NV_FALSE;
-    NvU32 page_size = NVRM_P2P_PAGESIZE_BIG_64K;
-    NvU32 page_size_index;
-    NvU64 temp_length;
-    NvU8 *gpu_uuid = NULL;
-    NvU8 uuid[NVIDIA_P2P_GPU_UUID_LEN] = {0};
-    int rc;
-
-    rc = nv_kmem_cache_alloc_stack(&sp);
-    if (rc != 0)
-    {
-        return rc;
-    }
-
-    *page_table = NULL;
-    status = os_alloc_mem((void **)&mem_info, sizeof(*mem_info));
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-
-    memset(mem_info, 0, sizeof(*mem_info));
-
-    INIT_LIST_HEAD(&mem_info->dma_mapping_list.list_head);
-    NV_INIT_MUTEX(&mem_info->dma_mapping_list.lock);
-
-    *page_table = &(mem_info->page_table);
-
-    mem_info->bPersistent = (free_callback == NULL);
-
-    //asign length to temporary variable since do_div macro does in-place division
-    temp_length = length;
-    do_div(temp_length, page_size);
-    page_count = temp_length;
-
-    if (length & (page_size - 1))
-    {
-        page_count++;
-    }
-
-    status = os_alloc_mem((void **)&physical_addresses,
-            (page_count * sizeof(NvU64)));
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-    status = os_alloc_mem((void **)&wreqmb_h, (page_count * sizeof(NvU32)));
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-    status = os_alloc_mem((void **)&rreqmb_h, (page_count * sizeof(NvU32)));
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-
-    if (mem_info->bPersistent)
-    {
-        void *gpu_info = NULL;
-
-        if ((p2p_token != 0) || (va_space != 0))
-        {
-            status = -ENOTSUPP;
-            goto failed;
-        }
-
-        status = rm_p2p_get_gpu_info(sp, virtual_address, length, &gpu_uuid, &gpu_info);
-        if (status != NV_OK)
-        {
-            goto failed;
-        }
-
-        rc = nvidia_dev_get_uuid(gpu_uuid, sp);
-        if (rc != 0)
-        {
-            status = NV_ERR_GPU_UUID_NOT_FOUND;
-            goto failed;
-        }
-
-        os_mem_copy(uuid, gpu_uuid, NVIDIA_P2P_GPU_UUID_LEN);
-
-        bGetUuid = NV_TRUE;
-
-        status = rm_p2p_get_pages_persistent(sp, virtual_address, length, &mem_info->private,
-                                             physical_addresses, &entries, *page_table, gpu_info);
-        if (status != NV_OK)
-        {
-            goto failed;
-        }
-    }
-    else
-    {
-        // Get regular old-style, non-persistent mappings
-        status = rm_p2p_get_pages(sp, p2p_token, va_space,
-                virtual_address, length, physical_addresses, wreqmb_h,
-                rreqmb_h, &entries, &gpu_uuid, *page_table);
-        if (status != NV_OK)
-        {
-            goto failed;
-        }
-    }
-
-    bGetPages = NV_TRUE;
-    (*page_table)->gpu_uuid = gpu_uuid;
-
-    status = os_alloc_mem((void *)&(*page_table)->pages,
-             (entries * sizeof(page)));
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-
-    (*page_table)->version = NVIDIA_P2P_PAGE_TABLE_VERSION;
-
-    for (i = 0; i < entries; i++)
-    {
-        page = NV_KMEM_CACHE_ALLOC(nvidia_p2p_page_t_cache);
-        if (page == NULL)
-        {
-            status = NV_ERR_NO_MEMORY;
-            goto failed;
-        }
-
-        memset(page, 0, sizeof(*page));
-
-        page->physical_address = physical_addresses[i];
-        page->registers.fermi.wreqmb_h = wreqmb_h[i];
-        page->registers.fermi.rreqmb_h = rreqmb_h[i];
-
-        (*page_table)->pages[i] = page;
-        (*page_table)->entries++;
-    }
-
-    status = nvidia_p2p_map_page_size(page_size, &page_size_index);
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-
-    (*page_table)->page_size = page_size_index;
-
-    os_free_mem(physical_addresses);
-    os_free_mem(wreqmb_h);
-    os_free_mem(rreqmb_h);
-
-    if (free_callback != NULL)
-    {
-        mem_info->free_callback = free_callback;
-        mem_info->data          = data;
-
-        status = rm_p2p_register_callback(sp, p2p_token, virtual_address, length,
-                                          *page_table, nv_p2p_mem_info_free_callback, mem_info);
-        if (status != NV_OK)
-        {
-            goto failed;
-        }
-    }
-
-    nv_kmem_cache_free_stack(sp);
-
-    return nvidia_p2p_map_status(status);
-
-failed:
-    if (physical_addresses != NULL)
-    {
-        os_free_mem(physical_addresses);
-    }
-    if (wreqmb_h != NULL)
-    {
-        os_free_mem(wreqmb_h);
-    }
-    if (rreqmb_h != NULL)
-    {
-        os_free_mem(rreqmb_h);
-    }
-
-    if (bGetPages)
-    {
-        (void)nv_p2p_put_pages(sp, p2p_token, va_space,
-                               virtual_address, page_table);
-    }
-
-    if (bGetUuid)
-    {
-        nvidia_dev_put_uuid(uuid, sp);
-    }
-
-    if (*page_table != NULL)
-    {
-        nv_p2p_free_page_table(*page_table);
-    }
-
-    nv_kmem_cache_free_stack(sp);
-
-    return nvidia_p2p_map_status(status);
-}
-
-EXPORT_SYMBOL(nvidia_p2p_get_pages);
-
-/*
- * This function is a no-op, but is left in place (for now), in order to allow
- * third-party callers to build and run without errors or warnings. This is OK,
- * because the missing functionality is provided by nv_p2p_free_platform_data,
- * which is being called as part of the RM's cleanup path.
- */
-int nvidia_p2p_free_page_table(struct nvidia_p2p_page_table *page_table)
-{
-    return 0;
-}
-
-EXPORT_SYMBOL(nvidia_p2p_free_page_table);
-
-int nvidia_p2p_put_pages(
-    uint64_t p2p_token,
-    uint32_t va_space,
-    uint64_t virtual_address,
-    struct nvidia_p2p_page_table *page_table
-)
-{
-    struct nv_p2p_mem_info *mem_info = NULL;
-    NvU8 uuid[NVIDIA_P2P_GPU_UUID_LEN] = {0};
-    NV_STATUS status;
-    nvidia_stack_t *sp = NULL;
-    int rc = 0;
-
-    os_mem_copy(uuid, page_table->gpu_uuid, NVIDIA_P2P_GPU_UUID_LEN);
-
-    mem_info = container_of(page_table, nv_p2p_mem_info_t, page_table);
-
-    rc = nv_kmem_cache_alloc_stack(&sp);
-    if (rc != 0)
-    {
-        return -ENOMEM;
-    }
-
-    status = nv_p2p_put_pages(sp, p2p_token, va_space,
-                              virtual_address, &page_table);
-
-    if (mem_info->bPersistent)
-    {
-        nvidia_dev_put_uuid(uuid, sp);
-    }
-
-    nv_kmem_cache_free_stack(sp);
-
-    return nvidia_p2p_map_status(status);
-}
-
-EXPORT_SYMBOL(nvidia_p2p_put_pages);
-
-int nvidia_p2p_dma_map_pages(
-    struct pci_dev *peer,
-    struct nvidia_p2p_page_table *page_table,
-    struct nvidia_p2p_dma_mapping **dma_mapping
-)
-{
-    NV_STATUS status;
-    nv_dma_device_t peer_dma_dev = {{ 0 }};
-    nvidia_stack_t *sp = NULL;
-    NvU64 *dma_addresses = NULL;
-    NvU32 page_count;
-    NvU32 page_size;
-    enum nvidia_p2p_page_size_type page_size_type;
-    struct nv_p2p_mem_info *mem_info = NULL;
-    NvU32 i;
-    void *priv;
-    int rc;
-
-    if (peer == NULL || page_table == NULL || dma_mapping == NULL ||
-        page_table->gpu_uuid == NULL)
-    {
-        return -EINVAL;
-    }
-
-    mem_info = container_of(page_table, nv_p2p_mem_info_t, page_table);
-
-    rc = nv_kmem_cache_alloc_stack(&sp);
-    if (rc != 0)
-    {
-        return rc;
-    }
-
-    *dma_mapping = NULL;
-    status = os_alloc_mem((void **)dma_mapping, sizeof(**dma_mapping));
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-    memset(*dma_mapping, 0, sizeof(**dma_mapping));
-
-    page_count = page_table->entries;
-
-    status = os_alloc_mem((void **)&dma_addresses,
-            (page_count * sizeof(NvU64)));
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-
-    page_size_type = page_table->page_size;
-
-    BUG_ON((page_size_type <= NVIDIA_P2P_PAGE_SIZE_4KB) ||
-           (page_size_type >= NVIDIA_P2P_PAGE_SIZE_COUNT));
-
-    peer_dma_dev.dev = &peer->dev;
-    peer_dma_dev.addressable_range.limit = peer->dma_mask;
-
-    page_size = nvidia_p2p_page_size_mappings[page_size_type];
-
-    for (i = 0; i < page_count; i++)
-    {
-        dma_addresses[i] = page_table->pages[i]->physical_address;
-    }
-
-    status = rm_p2p_dma_map_pages(sp, &peer_dma_dev,
-            page_table->gpu_uuid, page_size, page_count, dma_addresses, &priv);
-    if (status != NV_OK)
-    {
-        goto failed;
-    }
-
-    (*dma_mapping)->version = NVIDIA_P2P_DMA_MAPPING_VERSION;
-    (*dma_mapping)->page_size_type = page_size_type;
-    (*dma_mapping)->entries = page_count;
-    (*dma_mapping)->dma_addresses = dma_addresses;
-    (*dma_mapping)->private = priv;
-    (*dma_mapping)->pci_dev = peer;
-
-    /*
-     * All success, it is safe to insert dma_mapping now.
-     */
-    status = nv_p2p_insert_dma_mapping(mem_info, *dma_mapping);
-    if (status != NV_OK)
-    {
-        goto failed_insert;
-    }
-
-    nv_kmem_cache_free_stack(sp);
-
-    return 0;
-
-failed_insert:
-    nv_p2p_free_dma_mapping(*dma_mapping);
-    dma_addresses = NULL;
-    *dma_mapping = NULL;
-
-failed:
-    if (dma_addresses != NULL)
-    {
-        os_free_mem(dma_addresses);
-    }
-
-    if (*dma_mapping != NULL)
-    {
-        os_free_mem(*dma_mapping);
-        *dma_mapping = NULL;
-    }
-
-    nv_kmem_cache_free_stack(sp);
-
-    return nvidia_p2p_map_status(status);
-}
-
-EXPORT_SYMBOL(nvidia_p2p_dma_map_pages);
-
-int nvidia_p2p_dma_unmap_pages(
-    struct pci_dev *peer,
-    struct nvidia_p2p_page_table *page_table,
-    struct nvidia_p2p_dma_mapping *dma_mapping
-)
-{
-    struct nv_p2p_mem_info *mem_info = NULL;
-
-    if (peer == NULL || dma_mapping == NULL || page_table == NULL)
-    {
-        return -EINVAL;
-    }
-
-    mem_info = container_of(page_table, nv_p2p_mem_info_t, page_table);
-
-    /*
-     * nv_p2p_remove_dma_mapping returns dma_mapping if the dma_mapping was
-     * found and got unlinked from the mem_info->dma_mapping_list (atomically).
-     * This ensures that the RM's tear-down path does not race with this path.
-     *
-     * nv_p2p_remove_dma_mappings returns NULL if the dma_mapping was already
-     * unlinked.
-     */
-    if (nv_p2p_remove_dma_mapping(mem_info, dma_mapping) == NULL)
-    {
-        return 0;
-    }
-
-    WARN_ON(peer != dma_mapping->pci_dev);
-
-    BUG_ON((dma_mapping->page_size_type <= NVIDIA_P2P_PAGE_SIZE_4KB) ||
-           (dma_mapping->page_size_type >= NVIDIA_P2P_PAGE_SIZE_COUNT));
-
-    nv_p2p_free_dma_mapping(dma_mapping);
-
-    return 0;
-}
-
-EXPORT_SYMBOL(nvidia_p2p_dma_unmap_pages);
-
-/*
- * This function is a no-op, but is left in place (for now), in order to allow
- * third-party callers to build and run without errors or warnings. This is OK,
- * because the missing functionality is provided by nv_p2p_free_platform_data,
- * which is being called as part of the RM's cleanup path.
- */
-int nvidia_p2p_free_dma_mapping(
-    struct nvidia_p2p_dma_mapping *dma_mapping
-)
-{
-    return 0;
-}
-
-EXPORT_SYMBOL(nvidia_p2p_free_dma_mapping);
-
 int nvidia_p2p_register_rsync_driver(
     nvidia_p2p_rsync_driver_t *driver,
     void *data
